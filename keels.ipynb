{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3db4238-9f1f-4b14-9cc4-f88f4456b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests beautifulsoup4 pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1dbdae2-cf25-46dc-a44e-d7bdcad37687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 60 snack items. Saved to 'keells_snacks.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open snacks page\n",
    "driver.get(\"https://www.keellssuper.com/snacks\")\n",
    "time.sleep(3)  # Initial load\n",
    "\n",
    "# Scroll to load all products\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # wait for more products to load\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Parse page source\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "\n",
    "# Extract products\n",
    "products = soup.find_all(\"div\", class_=\"product-card-container\")\n",
    "data = []\n",
    "\n",
    "for product in products:\n",
    "    name_tag = product.find(\"div\", class_=\"product-card-name btn col-md-12\")\n",
    "    price_tag = product.find(\"div\", class_=\"product-card-final-price\")\n",
    "    \n",
    "    if name_tag and price_tag:\n",
    "        name = name_tag.get_text(strip=True)\n",
    "        price = price_tag.get_text(strip=True)\n",
    "        data.append({\n",
    "            \"Product Name\": name,\n",
    "            \"Price\": price\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"keells_snacks.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Scraped {len(data)} snack items. Saved to 'keells_snacks.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62edd7a3-4278-47e8-af94-b60a6822a45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 150 snack items. Saved to 'keells_fresh.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open snacks page\n",
    "driver.get(\"https://www.keellssuper.com/fresh-vegetables\")\n",
    "time.sleep(3)  # Initial load\n",
    "\n",
    "# Scroll to load all products\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # wait for more products to load\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Parse page source\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "\n",
    "# Extract products\n",
    "products = soup.find_all(\"div\", class_=\"product-card-container\")\n",
    "data = []\n",
    "\n",
    "for product in products:\n",
    "    name_tag = product.find(\"div\", class_=\"product-card-name btn col-md-12\")\n",
    "    price_tag = product.find(\"div\", class_=\"product-card-final-price\")\n",
    "    \n",
    "    if name_tag and price_tag:\n",
    "        name = name_tag.get_text(strip=True)\n",
    "        price = price_tag.get_text(strip=True)\n",
    "        data.append({\n",
    "            \"Product Name\": name,\n",
    "            \"Price\": price\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"keells_fresh.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Scraped {len(data)} snack items. Saved to 'keells_fresh.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5128ab43-7b2c-4d62-930b-4c9c2adbaa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 65 fruit items. Saved to 'keells_fruits.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Load the fresh fruits page\n",
    "driver.get(\"https://www.keellssuper.com/fresh-fruits\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Scroll to the bottom to load all products\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Parse the page\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "\n",
    "# Extract product cards\n",
    "products = soup.find_all(\"div\", class_=\"product-card-container\")\n",
    "\n",
    "# Parse and collect data\n",
    "data = []\n",
    "\n",
    "for product in products:\n",
    "    name_tag = product.find(\"div\", class_=\"product-card-name btn col-md-12\")\n",
    "    price_tag = product.find(\"div\", class_=\"product-card-final-price\")\n",
    "    image_tag = product.find(\"img\", class_=\"img-fluid\")\n",
    "\n",
    "    if name_tag and price_tag and image_tag:\n",
    "        name = name_tag.get_text(strip=True)\n",
    "        price = price_tag.get_text(strip=True)\n",
    "        image_url = image_tag['src']\n",
    "        data.append({\n",
    "            \"Product Name\": name,\n",
    "            \"Price\": price\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"keells_fruits.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Scraped {len(data)} fruit items. Saved to 'keells_fruits.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec8d096a-fa04-4776-acc3-18b2d7645f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 88 snack items. Saved to 'keells_meat.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open snacks page\n",
    "driver.get(\"https://www.keellssuper.com/keells-meat-shop\")\n",
    "time.sleep(3)  # Initial load\n",
    "\n",
    "# Scroll to load all products\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # wait for more products to load\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Parse page source\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "\n",
    "# Extract products\n",
    "products = soup.find_all(\"div\", class_=\"product-card-container\")\n",
    "data = []\n",
    "\n",
    "for product in products:\n",
    "    name_tag = product.find(\"div\", class_=\"product-card-name btn col-md-12\")\n",
    "    price_tag = product.find(\"div\", class_=\"product-card-final-price\")\n",
    "    \n",
    "    if name_tag and price_tag:\n",
    "        name = name_tag.get_text(strip=True)\n",
    "        price = price_tag.get_text(strip=True)\n",
    "        data.append({\n",
    "            \"Product Name\": name,\n",
    "            \"Price\": price\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"keells_meat.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Scraped {len(data)} snack items. Saved to 'keells_meat.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6d5815f-257a-4ec5-953a-943a3a01f093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 30 snack items. Saved to 'keells_fish.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open snacks page\n",
    "driver.get(\"https://www.keellssuper.com/fresh-fish\")\n",
    "time.sleep(3)  # Initial load\n",
    "\n",
    "# Scroll to load all products\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # wait for more products to load\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Parse page source\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "\n",
    "# Extract products\n",
    "products = soup.find_all(\"div\", class_=\"product-card-container\")\n",
    "data = []\n",
    "\n",
    "for product in products:\n",
    "    name_tag = product.find(\"div\", class_=\"product-card-name btn col-md-12\")\n",
    "    price_tag = product.find(\"div\", class_=\"product-card-final-price\")\n",
    "    \n",
    "    if name_tag and price_tag:\n",
    "        name = name_tag.get_text(strip=True)\n",
    "        price = price_tag.get_text(strip=True)\n",
    "        data.append({\n",
    "            \"Product Name\": name,\n",
    "            \"Price\": price\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"keells_fish.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Scraped {len(data)} snack items. Saved to 'keells_fish.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "720e28e2-322d-43b5-854e-f40598278243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 712 snack items. Saved to 'keells_beverages.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open snacks page\n",
    "driver.get(\"https://www.keellssuper.com/beverages\")\n",
    "time.sleep(3)  # Initial load\n",
    "\n",
    "# Scroll to load all products\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # wait for more products to load\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Parse page source\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "\n",
    "# Extract products\n",
    "products = soup.find_all(\"div\", class_=\"product-card-container\")\n",
    "data = []\n",
    "\n",
    "for product in products:\n",
    "    name_tag = product.find(\"div\", class_=\"product-card-name btn col-md-12\")\n",
    "    price_tag = product.find(\"div\", class_=\"product-card-final-price\")\n",
    "    \n",
    "    if name_tag and price_tag:\n",
    "        name = name_tag.get_text(strip=True)\n",
    "        price = price_tag.get_text(strip=True)\n",
    "        data.append({\n",
    "            \"Product Name\": name,\n",
    "            \"Price\": price\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"keells_beverages.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Scraped {len(data)} snack items. Saved to 'keells_beverages.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae6a22dd-acc0-4ddb-b33d-3445102c5ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 100 chilled items. Saved to 'keells_chilled_products.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Load the chilled products page\n",
    "driver.get(\"https://www.keellssuper.com/chilled-products\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Scroll to load all products\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Parse the page\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "\n",
    "# Extract product cards\n",
    "products = soup.find_all(\"div\", class_=\"product-card-container\")\n",
    "\n",
    "# Parse and collect data\n",
    "data = []\n",
    "\n",
    "for product in products:\n",
    "    name_tag = product.find(\"div\", class_=\"product-card-name btn col-md-12\")\n",
    "    final_price_tag = product.find(\"div\", class_=\"product-card-final-price\")\n",
    "    original_price_tag = product.find(\"div\", class_=\"product-card-original-price\")\n",
    "    discount_tag = product.find(\"div\", class_=\"product-card-promotion-badge-percentage\")\n",
    "    image_tag = product.find(\"img\", class_=\"img-fluid\")\n",
    "\n",
    "    name = name_tag.get_text(strip=True) if name_tag else \"N/A\"\n",
    "    final_price = final_price_tag.get_text(strip=True) if final_price_tag else \"N/A\"\n",
    "    original_price = original_price_tag.get_text(strip=True) if original_price_tag else \"\"\n",
    "    discount = discount_tag.get_text(strip=True) + \"%\" if discount_tag else \"\"\n",
    "    image_url = image_tag['src'] if image_tag and 'http' in image_tag['src'] else \"https://www.keellssuper.com\" + image_tag['src'] if image_tag else \"\"\n",
    "\n",
    "    data.append({\n",
    "        \"Product Name\": name,\n",
    "        \"Final Price\": final_price,\n",
    "        \"Original Price\": original_price,\n",
    "        \"Discount\": discount\n",
    "    })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"keells_chilled_products.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Scraped {len(data)} chilled items. Saved to 'keells_chilled_products.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19a9e42f-416d-4f11-a8c0-afdd7c1f07e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 387 snack items. Saved to 'keells_frozen.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open snacks page\n",
    "driver.get(\"https://www.keellssuper.com/frozen-food\")\n",
    "time.sleep(3)  # Initial load\n",
    "\n",
    "# Scroll to load all products\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # wait for more products to load\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Parse page source\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "\n",
    "# Extract products\n",
    "products = soup.find_all(\"div\", class_=\"product-card-container\")\n",
    "data = []\n",
    "\n",
    "for product in products:\n",
    "    name_tag = product.find(\"div\", class_=\"product-card-name btn col-md-12\")\n",
    "    price_tag = product.find(\"div\", class_=\"product-card-final-price\")\n",
    "    \n",
    "    if name_tag and price_tag:\n",
    "        name = name_tag.get_text(strip=True)\n",
    "        price = price_tag.get_text(strip=True)\n",
    "        data.append({\n",
    "            \"Product Name\": name,\n",
    "            \"Price\": price\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"keells_frozen.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Scraped {len(data)} snack items. Saved to 'keells_frozen.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd63c20e-c4e3-4a40-bba0-110561317bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 600 grocery items. Saved to 'keells_grocery.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Headless Chrome setup\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open grocery page\n",
    "driver.get(\"https://www.keellssuper.com/grocery\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Scroll to load all items\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Parse page source\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "\n",
    "# Extract product containers\n",
    "products = soup.find_all(\"div\", class_=\"product-card-container\")\n",
    "\n",
    "# Store data\n",
    "data = []\n",
    "\n",
    "for product in products:\n",
    "    name_tag = product.find(\"div\", class_=\"product-card-name btn col-md-12\")\n",
    "    price_tag = product.find(\"div\", class_=\"product-card-final-price\")\n",
    "    image_tag = product.find(\"img\", class_=\"img-fluid\")\n",
    "\n",
    "    if name_tag and price_tag:\n",
    "        name = name_tag.get_text(strip=True)\n",
    "        price = price_tag.get_text(strip=True)\n",
    "        image_url = image_tag['src'] if image_tag and 'http' in image_tag['src'] else \"https://www.keellssuper.com\" + image_tag['src'] if image_tag else \"\"\n",
    "\n",
    "        data.append({\n",
    "            \"Product Name\": name,\n",
    "            \"Price\": price\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"keells_grocery.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Scraped {len(data)} grocery items. Saved to 'keells_grocery.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26ab8fd8-5bda-4b86-9bed-d7bdb47ffde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 20 household items. Saved to 'keells_household_essentials.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Load the household essentials page\n",
    "driver.get(\"https://www.keellssuper.com/household-essentials\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Scroll to load all products\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Parse HTML\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "\n",
    "# Find all product containers\n",
    "products = soup.find_all(\"div\", class_=\"product-card-container\")\n",
    "\n",
    "# Store scraped data\n",
    "data = []\n",
    "\n",
    "for product in products:\n",
    "    name_tag = product.find(\"div\", class_=\"product-card-name btn col-md-12\")\n",
    "    final_price_tag = product.find(\"div\", class_=\"product-card-final-price\")\n",
    "    original_price_tag = product.find(\"div\", class_=\"product-card-original-price\")\n",
    "    discount_tag = product.find(\"div\", class_=\"product-card-promotion-badge-percentage\")\n",
    "    image_tag = product.find(\"img\", class_=\"img-fluid\")\n",
    "\n",
    "    name = name_tag.get_text(strip=True) if name_tag else \"N/A\"\n",
    "    final_price = final_price_tag.get_text(strip=True) if final_price_tag else \"\"\n",
    "    original_price = original_price_tag.get_text(strip=True) if original_price_tag else \"\"\n",
    "    discount = discount_tag.get_text(strip=True) + \"%\" if discount_tag else \"\"\n",
    "    image_url = image_tag['src'] if image_tag and \"http\" in image_tag['src'] else \"https://www.keellssuper.com\" + image_tag['src'] if image_tag else \"\"\n",
    "\n",
    "    data.append({\n",
    "        \"Product Name\": name,\n",
    "        \"Final Price\": final_price,\n",
    "        \"Original Price\": original_price,\n",
    "        \"Discount\": discount\n",
    "    })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"keells_household_essentials.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Scraped {len(data)} household items. Saved to 'keells_household_essentials.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17009830-5a9e-47b4-8dbe-2e72d218b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 29 bakery items. Saved to 'keells_bakery.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Visit the bakery page\n",
    "driver.get(\"https://www.keellssuper.com/keells-bakery\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Scroll to load all items\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Parse HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "\n",
    "# Extract product cards\n",
    "products = soup.find_all(\"div\", class_=\"product-card-container\")\n",
    "\n",
    "# Collect product data\n",
    "data = []\n",
    "\n",
    "for product in products:\n",
    "    name_tag = product.find(\"div\", class_=\"product-card-name btn col-md-12\")\n",
    "    final_price_tag = product.find(\"div\", class_=\"product-card-final-price\")\n",
    "    image_tag = product.find(\"img\", class_=\"img-fluid\")\n",
    "\n",
    "    if name_tag and final_price_tag and image_tag:\n",
    "        name = name_tag.get_text(strip=True)\n",
    "        price = final_price_tag.get_text(strip=True)\n",
    "        image_url = image_tag['src'] if 'http' in image_tag['src'] else \"https://www.keellssuper.com\" + image_tag['src']\n",
    "\n",
    "        data.append({\n",
    "            \"Product Name\": name,\n",
    "            \"Final Price\": price\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"keells_bakery.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Scraped {len(data)} bakery items. Saved to 'keells_bakery.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc0e4517-125b-4294-99d0-542622d3d24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 55 electronic items. Saved to 'keells_electronics.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Load the electronics page\n",
    "driver.get(\"https://www.keellssuper.com/electronic-devices\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Scroll to load all items\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Parse page content\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "\n",
    "# Find all product containers\n",
    "products = soup.find_all(\"div\", class_=\"product-card-container\")\n",
    "\n",
    "# Collect product data\n",
    "data = []\n",
    "\n",
    "for product in products:\n",
    "    name_tag = product.find(\"div\", class_=\"product-card-name btn col-md-12\")\n",
    "    final_price_tag = product.find(\"div\", class_=\"product-card-final-price\")\n",
    "    image_tag = product.find(\"img\", class_=\"img-fluid\")\n",
    "\n",
    "    if name_tag and final_price_tag and image_tag:\n",
    "        name = name_tag.get_text(strip=True)\n",
    "        price = final_price_tag.get_text(strip=True)\n",
    "        image_url = image_tag['src'] if \"http\" in image_tag['src'] else \"https://www.keellssuper.com\" + image_tag['src']\n",
    "\n",
    "        data.append({\n",
    "            \"Product Name\": name,\n",
    "            \"Final Price\": price,\n",
    "            \"Image URL\": image_url\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"keells_electronics.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Scraped {len(data)} electronic items. Saved to 'keells_electronics.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad98540f-fec0-4a68-a52c-76a2c0a48c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 8 snack items. Saved to 'keells_hamper.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open snacks page\n",
    "driver.get(\"https://www.keellssuper.com/hampers-and-vouchers\")\n",
    "time.sleep(3)  # Initial load\n",
    "\n",
    "# Scroll to load all products\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # wait for more products to load\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Parse page source\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "\n",
    "# Extract products\n",
    "products = soup.find_all(\"div\", class_=\"product-card-container\")\n",
    "data = []\n",
    "\n",
    "for product in products:\n",
    "    name_tag = product.find(\"div\", class_=\"product-card-name btn col-md-12\")\n",
    "    price_tag = product.find(\"div\", class_=\"product-card-final-price\")\n",
    "    \n",
    "    if name_tag and price_tag:\n",
    "        name = name_tag.get_text(strip=True)\n",
    "        price = price_tag.get_text(strip=True)\n",
    "        data.append({\n",
    "            \"Product Name\": name,\n",
    "            \"Price\": price\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"keells_hamper.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Scraped {len(data)} snack items. Saved to 'keells_hamper.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e1998-cff8-47dc-9c58-697de02d9d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
